{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ad95d",
   "metadata": {},
   "source": [
    "## Initialize The Deep neural network Module\n",
    "Here we use a pretrained model weights and architecture  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the weights file\n",
    "model_weights =  \"model/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "\n",
    "# Path to the architecture file\n",
    "model_arch = \"model/deploy.prototxt.txt\"\n",
    "\n",
    "# Load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch, model_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b8519",
   "metadata": {},
   "source": [
    "## Built architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold =0.7):\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Apply mean subtraction, and create 4D blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0,(300, 300), (104.0, 117.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "\n",
    "    faces = net.forward()\n",
    "\n",
    "    # Get the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "\n",
    "    # Get the index of the prediction with highest confidence and get its face\n",
    "    index = np.argmax(prediction_scores)\n",
    "    face = faces[0,0,index]\n",
    "    confidence = face[2]\n",
    "\n",
    "    if confidence > threshold:\n",
    "        # The 4 values at indexes 3 to 6 are the top-left, bottom-right coordinates\n",
    "        # scales to range 0-1.The original coordinates can be found by\n",
    "        # multiplying x,y values with the width,height of the image\n",
    "        box = face[3:7] * np.array([width, height, width, height])\n",
    "\n",
    "        # The coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therefore needs be quantized to int type\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "\n",
    "        annotated_frame = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        output = (annotated_frame, (x1, y1, x2, y2), True, confidence)\n",
    "        \n",
    "    else:\n",
    "        output = (image,(),False, 0)\n",
    "     \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d33ad4",
   "metadata": {},
   "source": [
    "## our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image):\n",
    "\n",
    "    # Create a face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Run detector and get bounding boxes of the faces on image.\n",
    "    detected_faces = face_detector(image, 1)\n",
    "    face_frames = [(x.left(), x.top(),\n",
    "                    x.right(), x.bottom()) for x in detected_faces]\n",
    "\n",
    "    return face_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a91d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def Face_detection(frame):\n",
    "    # Load image\n",
    "    #image = io.imread(img_path)\n",
    "    # Detect faces\n",
    "    detected_faces = detect_faces(frame)\n",
    "\n",
    "    # Crop faces and plot\n",
    "    for n, face_rect in enumerate(detected_faces):\n",
    "        topleft = (face_rect[0],face_rect[1])\n",
    "        bottomright = (face_rect[2],face_rect[3])\n",
    "        annotated_frame = cv2.rectangle(frame.copy(), topleft, bottomright, (0, 0, 255), 2)\n",
    "        coords = (face_rect[0],face_rect[1],face_rect[2],face_rect[3])\n",
    "        #face = Image.fromarray(annotated_frame)#.crop(face_rect)\n",
    "        #print(face_rect)\n",
    "    return annotated_frame,coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea7e817",
   "metadata": {},
   "source": [
    "## webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71edd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "# define the `Detection` object\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51130486",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayofIOU = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('face Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame \"ground-truth\"\n",
    "    annotated_frame, coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    #OUR MODEL\n",
    "    face,cooords = Face_detection(annotated_frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('face Detection',face)\n",
    "    \n",
    "    #saveimage\n",
    "    filename = '/home/mora/Downloads/FinalCV/imageiou/image{}.jpg'.format(str(counter))\n",
    "    cv2.imwrite(filename, face)\n",
    "    counter += 1\n",
    "    \n",
    "    #create image tuple\n",
    "    det = Detection(filename, cooords, coords)\n",
    "    arrayofIOU.append(det)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097d59e",
   "metadata": {},
   "source": [
    "## IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayofIOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the example detections\n",
    "for detection in arrayofIOU:\n",
    "    # load the image\n",
    "    image = cv2.imread(detection.image_path)\n",
    "    \n",
    "    iou = bb_intersection_over_union(detection.gt, detection.pred)\n",
    "    cv2.putText(image, \"IoU: {:.4f}\".format(iou), (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    #print(\"{}: {:.4f}\".format(detection.image_path, iou))\n",
    "    # show the output image\n",
    "    #cv2.imshow(\"Image\", image)\n",
    "    #saveimage\n",
    "    filename = detection.image_path\n",
    "    cv2.imwrite(filename, image)\n",
    "    \n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956da5a",
   "metadata": {},
   "source": [
    "## Landmarks Detection\n",
    "In this part we use the pretrained model from dlib library that can detect 68 keypoints of the face as the one we buit has a very low accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(\"model/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d91a21",
   "metadata": {},
   "source": [
    "First we define shapt_to_np function that creates an np array of shape (68, 2) for storing the landmark coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182686d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape):\n",
    "    # Create an array of shape (68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype=\"int\")\n",
    "\n",
    "    # Write the x,y coordinates of each landmark into the array\n",
    "    for i in range(0, 68):\n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254ed81",
   "metadata": {},
   "source": [
    "Then we create the function to detect the facial landmarks. This function takes the box of the face detected by the face_detecor() function and the image as inputs and return the image with the landmarks on it and an np array of their positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(box, image):\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Get the coordinates and apply the detection method\n",
    "    (x1, y1, x2, y2) = box\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x1, y1, x2, y2))\n",
    "\n",
    "    # Convert the shape into np array and draw the landmarks on the image\n",
    "    landmarks = shape_to_np(shape)\n",
    "    for (x, y) in landmarks:\n",
    "        annotated_image = cv2.circle(image, (x, y),2, (0, 127, 255), -1)\n",
    "\n",
    "    return annotated_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16abd4",
   "metadata": {},
   "source": [
    "Testing the detect_landmarks() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801639a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam and set the window to normal\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('Landmark Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # If frame is returned, flip and detect the face\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "\n",
    "    # Get the landmarks of the face detected\n",
    "    if status:\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "    cv2.imshow('Landmark Detection',landmark_image)\n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f87437",
   "metadata": {},
   "source": [
    "## Jump Control mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44ddaa",
   "metadata": {},
   "source": [
    "**determine if mouth open through calculating aspect ratio between length and width, and if bigger certain threshold, then mouth is open**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dab3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_open(landmarks, threshold = 0.7):\n",
    "    # Calculate the euclidean distance labelled as d1,d2,d3\n",
    "    d1 = hypot(landmarks[50][0] - landmarks[58][0], landmarks[50][1] - landmarks[58][1])\n",
    "    d2 = hypot(landmarks[52][0] - landmarks[56][0], landmarks[52][1] - landmarks[56][1])\n",
    "    d3 = hypot(landmarks[48][0] - landmarks[54][0], landmarks[48][1] - landmarks[54][1])\n",
    "\n",
    "    # Calculate the mouth aspect ratio\n",
    "    ratio = (d1 + d2) / (2.0 * d3)\n",
    "\n",
    "    # Return True if the value is greater than the threshold\n",
    "    if ratio > threshold:\n",
    "        return True, ratio\n",
    "    else:\n",
    "        return False, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Mouth Status', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "\n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "\n",
    "    if status:\n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "        # Adjust the threshold and make sure it's working for you.\n",
    "        mouth_status,_ = if_open(landmarks, 0.6)\n",
    "\n",
    "        # Display the mouth status\n",
    "        cv2.putText(frame,'Is Mouth Open: {}'.format(mouth_status),(20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "    # Display the frame\n",
    "    cv2.imshow('Mouth Status',frame)\n",
    "     \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032de54a",
   "metadata": {},
   "source": [
    "## Crouch Control Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ede2b6",
   "metadata": {},
   "source": [
    "**Determine how close the face is to camera, to capture movements of face nearer to camera , in which translated later into down button in game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562babb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_proximity(bounding_box,image, threshold = 325):\n",
    "\n",
    "    # Get the height and width of the face bounding box\n",
    "    f_width =  bounding_box[2]-bounding_box[0]\n",
    "    f_height = bounding_box[3]-bounding_box[1]\n",
    "    \n",
    "    # Draw rectangle to guide the user\n",
    "    # Calculate the angle of diagonal using face width, height\n",
    "    theta = np.arctan(f_height/f_width)\n",
    "\n",
    "    # Use the angle to calculate height, width of the guide rectangle\n",
    "    height = np.sin(theta)*threshold\n",
    "    width  = np.cos(theta)*threshold\n",
    "\n",
    "    # Calculate the mid-point of the guide rectangle/face bounding box\n",
    "    mid_x,mid_y = (bounding_box[2]+bounding_box[0])/2 , (bounding_box[3]+bounding_box[1])/2\n",
    "\n",
    "    # Calculate the coordinates of top-left and bottom-right corners\n",
    "    topleft = int(mid_x-(width/2)), int(mid_y-(height/2))\n",
    "    bottomright = int(mid_x +(width/2)), int(mid_y + (height/2))\n",
    "\n",
    "    # Draw the guide rectangle\n",
    "    cv2.rectangle(image, topleft, bottomright, (0, 255, 255), 2)\n",
    "\n",
    "    # Calculate the diagonal distance of the bounding box\n",
    "    diagonal = hypot(width, height)\n",
    "\n",
    "    # Return True if distance greater than the threshold\n",
    "    if diagonal > threshold:\n",
    "        return True, diagonal\n",
    "    else:\n",
    "        return False, diagonal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Face proximity', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "\n",
    "    if status:\n",
    "        # Check if face is closer than the defined threshold\n",
    "        is_face_close,_ = face_proximity(box_coords, face_image, 300)\n",
    "\n",
    "        # Display the mouth status\n",
    "        cv2.putText(face_image,'Is Face Close: {}'.format(is_face_close),(20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face proximity',face_image)\n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4a2de",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Calibration', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "\n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "\n",
    "    if status:\n",
    "        # Detect landmarks if the frame is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "        # Get the current mouth aspect ratio\n",
    "        _,mouth_ar = if_open(landmarks)\n",
    "\n",
    "        # Get the current face proximity\n",
    "        _, proximity  = face_proximity(box_coords, face_image)\n",
    "\n",
    "        # Calculate threshold values\n",
    "        ar_threshold = mouth_ar*1.4\n",
    "        proximity_threshold = proximity*1.3\n",
    "\n",
    "        # Dsiplay the threshold values\n",
    "        cv2.putText(frame, 'Aspect ratio threshold: {:.2f} '.format(ar_threshold),(20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        cv2.putText(frame,'Proximity threshold: {:.2f}'.format(proximity_threshold),(20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "    # Display the frame\n",
    "    cv2.imshow('Calibration',frame)\n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c7afb",
   "metadata": {},
   "source": [
    "## Keyboard Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e37d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will open a context menu\n",
    "pyautogui.click(button='right')\n",
    "\n",
    "# Press space bar. This will scroll down the page in some browsers\n",
    "pyautogui.press('space')\n",
    "\n",
    "# This will move the focus to the next cell in the notebook\n",
    "pyautogui.press(['shift','enter'])\n",
    "\n",
    "# Hold down the shift key\n",
    "pyautogui.keyDown('shift')\n",
    "\n",
    "# Press enter while the shift key is down, this will run the next code cell\n",
    "pyautogui.press('enter')\n",
    "\n",
    "# Release the shift key\n",
    "pyautogui.keyUp('shift')\n",
    "\n",
    "# This will run automatically after running the two code cells above\n",
    "print('I ran')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6a785",
   "metadata": {},
   "source": [
    "## Build The Final Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyautogui.FAILSAFE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Dino with OpenCV', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# By default each key press is followed by a 0.1 second pause\n",
    "pyautogui.PAUSE = 0.0\n",
    "\n",
    "# The fail-safe triggers an exception in case mouse\n",
    "# is moved to corner of the screen\n",
    "#pyautogui.FAILSAFE = False\n",
    "\n",
    "while(True):\n",
    "     # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "\n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "\n",
    "    if status:\n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "        # Check if mouth is open\n",
    "        is_open,_ = if_open(landmarks, ar_threshold)\n",
    "    \n",
    "        # If the mouth is open trigger space key Down event to jump\n",
    "        if is_open:\n",
    "            pyautogui.keyDown('space')\n",
    "            mouth_status = 'Open'\n",
    "        else:\n",
    "            # Else the space key is Up\n",
    "            pyautogui.keyUp('space')\n",
    "            mouth_status = 'Closed'\n",
    "    \n",
    "        # Display the mouth status on the frame\n",
    "        cv2.putText(frame,'Mouth: {}'.format(mouth_status),(20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        # Check the proximity of the face\n",
    "        is_closer,_  = face_proximity(box_coords, frame, proximity_threshold)\n",
    "\n",
    "        # If face is closer press the down key\n",
    "        if is_closer:\n",
    "            pyautogui.keyDown('down')\n",
    "        else:\n",
    "            pyautogui.keyUp('down')\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Dino with OpenCV',frame)\n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1508baa",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vidcap = cv2.VideoCapture('IMG_7492.MOV')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "  cv2.imwrite(\"./video_frames/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "  success,image = vidcap.read()\n",
    "  print('Read a new frame: ', success)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be0c10",
   "metadata": {},
   "source": [
    "## Claculating the second metric (frame per second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "frames = glob.glob('video_frames/*')\n",
    "number_of_frames = len(frames)\n",
    "\n",
    "print(f'the number of Frames are {number_of_frames}')\n",
    "\n",
    "predict_mouth = []\n",
    "\n",
    "for frame in frames:\n",
    "    frame = cv2.imread(frame)\n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Check if mouth is open\n",
    "        is_open,_ = is_mouth_open(landmarks, ar_threshold)\n",
    "\n",
    "        predict_mouth.append(is_open)\n",
    "        \n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "\n",
    "frame_per_sec = number_of_frames/elapsed\n",
    "frame_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'the model work with framerate {frame_per_sec} frame per second in cpu with 8 gb ram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means the mouth is closed , 1 means the mouth is open\n",
    "y_pred = predict_mouth\n",
    "\n",
    "for i in range(len(y_pred)) :\n",
    "    if y_pred[i] == False:\n",
    "        y_pred[i] = 0\n",
    "    else:\n",
    "        y_pred[i] = 1\n",
    "        \n",
    "\n",
    "true_label = [] #the first half of frames the mouth is closed, the second half the mouth is open\n",
    "\n",
    "for i in range(len(y_pred)) :\n",
    "    if i <= (len(y_pred)/2):\n",
    "        true_label.append(0)\n",
    "    else:\n",
    "        true_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095abaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(true_label, y_pred).ravel()\n",
    "\n",
    "print(f'The True Positive are  {tp}')\n",
    "print(f'The False Negative are {fn}')\n",
    "print(f'The False positive are {fp}')\n",
    "print(f'The true negative are  {tn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(true_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de620331",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "sb.heatmap(confusion_matrix(true_label, y_pred), \n",
    "           annot = False,\n",
    "           cmap = sb.color_palette(\"rocket\", as_cmap=True));\n",
    "plt.xticks(rotation = 45);\n",
    "plt.ylabel(\"True label\", ha=\"right\", rotation = 0, color = \"blue\");\n",
    "plt.xlabel(\"Predicted label\", color = \"blue\");\n",
    "plt.title(\"Test Data Confusion Matrix\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123a5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
